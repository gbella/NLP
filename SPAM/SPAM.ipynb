{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Install requirements"
      ],
      "metadata": {
        "id": "IIiuS3j6AF0K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from tabulate import tabulate\n",
        "from tqdm import trange\n",
        "import random"
      ],
      "metadata": {
        "id": "KunPb0y0S4uu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download annotated SPAM corpora"
      ],
      "metadata": {
        "id": "zVBpGs7GALSu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/gbella/NLP/raw/main/SPAM/spam_corpora.zip\n",
        "!unzip spam_corpora.zip"
      ],
      "metadata": {
        "id": "PkOzPa1PeTUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the two corpora and fuse them into a single dataset"
      ],
      "metadata": {
        "id": "OH5bIhlUATbA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('email_spam.csv')\n",
        "data2 = pd.read_csv('sms_spam.csv',sep=';')\n",
        "messages = pd.concat([data['message'],data2['message']]).values\n",
        "labels = pd.concat([data['label'],data2['label']]).values\n",
        "#test_messages = data['message'].values\n",
        "#test_labels = data['label'].values"
      ],
      "metadata": {
        "id": "fG-0yv_lX6Ux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocess data"
      ],
      "metadata": {
        "id": "yRw3uv2NAeaT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "token_id = []\n",
        "attention_masks = []\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case = False)\n",
        "\n",
        "def preprocessing(input_text, tokenizer):\n",
        "  '''\n",
        "  Returns <class transformers.tokenization_utils_base.BatchEncoding> with the following fields:\n",
        "    - input_ids: list of token ids\n",
        "    - token_type_ids: list of token type ids\n",
        "    - attention_mask: list of indices (0,1) specifying which tokens should be considered by the model (return_attention_mask = True).\n",
        "  '''\n",
        "  return tokenizer.encode_plus(\n",
        "                        input_text,\n",
        "                        add_special_tokens = True,\n",
        "                        max_length = 32,\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,\n",
        "                        return_tensors = 'pt'\n",
        "                   )\n",
        "\n",
        "for sample in messages:\n",
        "  encoding_dict = preprocessing(sample, tokenizer)\n",
        "  token_id.append(encoding_dict['input_ids'])\n",
        "  attention_masks.append(encoding_dict['attention_mask'])\n",
        "\n",
        "token_id = torch.cat(token_id, dim = 0)\n",
        "attention_masks = torch.cat(attention_masks, dim = 0)\n",
        "labels = torch.tensor(labels)"
      ],
      "metadata": {
        "id": "RBAZFeXiTQyB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_ratio = 0.2\n",
        "# Recommended batch size: 16, 32. See: https://arxiv.org/pdf/1810.04805.pdf\n",
        "batch_size = 16\n",
        "\n",
        "# Split the dataset into a training and a remaining set, according to val_ratio\n",
        "train_idx, rem_idx = train_test_split(\n",
        "    np.arange(len(labels)),\n",
        "    test_size = val_ratio,\n",
        "    shuffle = True)\n",
        "\n",
        "# Split again the remaining set into validation and test sets\n",
        "val_idx, test_idx = train_test_split(\n",
        "    np.arange(len(rem_idx)),\n",
        "    test_size = 0.5,\n",
        "    shuffle = True)\n",
        "\n",
        "train_set = TensorDataset(token_id[train_idx],\n",
        "                          attention_masks[train_idx],\n",
        "                          labels[train_idx])\n",
        "\n",
        "val_set = TensorDataset(token_id[val_idx],\n",
        "                        attention_masks[val_idx],\n",
        "                        labels[val_idx])\n",
        "\n",
        "test_set = TensorDataset(token_id[test_idx],\n",
        "                         attention_masks[test_idx],\n",
        "                         labels[test_idx])\n",
        "\n",
        "# Prepare DataLoader\n",
        "train_dataloader = DataLoader(\n",
        "            train_set,\n",
        "            sampler = RandomSampler(train_set),\n",
        "            batch_size = batch_size\n",
        "        )\n",
        "\n",
        "validation_dataloader = DataLoader(\n",
        "            val_set,\n",
        "            sampler = SequentialSampler(val_set),\n",
        "            batch_size = batch_size\n",
        "        )\n",
        "\n",
        "test_dataloader = DataLoader(\n",
        "            test_set,\n",
        "            sampler = SequentialSampler(test_set),\n",
        "            batch_size = batch_size\n",
        "        )"
      ],
      "metadata": {
        "id": "VfaArMwsTckU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute evaluation metrics: accuracy, precision, recall, F-measure\n",
        "\n",
        "def b_tp(preds, labels):\n",
        "  '''Returns True Positives (TP): count of correct predictions of actual class 1'''\n",
        "  return sum([preds == labels and preds == 1 for preds, labels in zip(preds, labels)])\n",
        "\n",
        "def b_fp(preds, labels):\n",
        "  '''Returns False Positives (FP): count of wrong predictions of actual class 1'''\n",
        "  return sum([preds != labels and preds == 1 for preds, labels in zip(preds, labels)])\n",
        "\n",
        "def b_tn(preds, labels):\n",
        "  '''Returns True Negatives (TN): count of correct predictions of actual class 0'''\n",
        "  return sum([preds == labels and preds == 0 for preds, labels in zip(preds, labels)])\n",
        "\n",
        "def b_fn(preds, labels):\n",
        "  '''Returns False Negatives (FN): count of wrong predictions of actual class 0'''\n",
        "  return sum([preds != labels and preds == 0 for preds, labels in zip(preds, labels)])\n",
        "\n",
        "def b_metrics(preds, labels):\n",
        "  '''\n",
        "  Returns the following metrics:\n",
        "    - accuracy    = (TP + TN) / N\n",
        "    - precision   = TP / (TP + FP)\n",
        "    - recall      = TP / (TP + FN)\n",
        "    - f1          = 2 * precision * recall / (precision + recall)\n",
        "  '''\n",
        "  preds = np.argmax(preds, axis = 1).flatten()\n",
        "  labels = labels.flatten()\n",
        "  tp = b_tp(preds, labels)\n",
        "  tn = b_tn(preds, labels)\n",
        "  fp = b_fp(preds, labels)\n",
        "  fn = b_fn(preds, labels)\n",
        "  b_accuracy = (tp + tn) / len(labels)\n",
        "  b_precision = tp / (tp + fp) if (tp + fp) > 0 else 'nan'\n",
        "  b_recall = tp / (tp + fn) if (tp + fn) > 0 else 'nan'\n",
        "  b_f1 = 2.0 * b_precision * b_recall / (b_precision + b_recall) if (not b_precision == 'nan') and (not b_recall == 'nan') and (b_precision + b_recall) > 0 else 'nan'\n",
        "  return b_accuracy, b_precision, b_recall, b_f1"
      ],
      "metadata": {
        "id": "rHjG_d0xTnPV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the BertForSequenceClassification model\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    'bert-base-cased',\n",
        "    num_labels = 2,\n",
        "    output_attentions = False,\n",
        "    output_hidden_states = False,\n",
        ")\n",
        "\n",
        "# Recommended learning rates (Adam): 5e-5, 3e-5, 2e-5. See: https://arxiv.org/pdf/1810.04805.pdf\n",
        "optimizer = torch.optim.AdamW(model.parameters(),\n",
        "                              lr = 5e-5,\n",
        "                              eps = 1e-08\n",
        "                              )\n",
        "\n",
        "# Run on GPU\n",
        "model.cuda()"
      ],
      "metadata": {
        "id": "BoH9A5URToEj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Recommended number of epochs: 2, 3, 4. See: https://arxiv.org/pdf/1810.04805.pdf\n",
        "epochs = 8\n",
        "\n",
        "for _ in trange(epochs, desc = 'Epoch'):\n",
        "\n",
        "    # ========== Training ==========\n",
        "\n",
        "    # Set model to training mode\n",
        "    model.train()\n",
        "\n",
        "    # Tracking variables\n",
        "    tr_loss = 0\n",
        "    nb_tr_examples, nb_tr_steps = 0, 0\n",
        "\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        optimizer.zero_grad()\n",
        "        # Forward pass\n",
        "        train_output = model(b_input_ids,\n",
        "                             token_type_ids = None,\n",
        "                             attention_mask = b_input_mask,\n",
        "                             labels = b_labels)\n",
        "        # Backward pass\n",
        "        train_output.loss.backward()\n",
        "        optimizer.step()\n",
        "        # Update tracking variables\n",
        "        tr_loss += train_output.loss.item()\n",
        "        nb_tr_examples += b_input_ids.size(0)\n",
        "        nb_tr_steps += 1\n",
        "\n",
        "    # ========== Validation ==========\n",
        "\n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables\n",
        "    val_accuracy = []\n",
        "    val_precision = []\n",
        "    val_recall = []\n",
        "    val_f1 = []\n",
        "\n",
        "    for batch in validation_dataloader:\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        with torch.no_grad():\n",
        "          # Forward pass\n",
        "          eval_output = model(b_input_ids,\n",
        "                              token_type_ids = None,\n",
        "                              attention_mask = b_input_mask)\n",
        "        logits = eval_output.logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        # Calculate validation metrics\n",
        "        b_accuracy, b_precision, b_recall, b_f1 = b_metrics(logits, label_ids)\n",
        "        val_accuracy.append(b_accuracy)\n",
        "        # Update precision only when (tp + fp) !=0; ignore nan\n",
        "        if b_precision != 'nan': val_precision.append(b_precision)\n",
        "        # Update recall only when (tp + fn) !=0; ignore nan\n",
        "        if b_recall != 'nan': val_recall.append(b_recall)\n",
        "        # Update specificity only when (tn + fp) !=0; ignore nan\n",
        "        if b_f1 != 'nan': val_f1.append(b_f1)\n",
        "\n",
        "    print('\\n\\t - Train loss: {:.4f}'.format(tr_loss / nb_tr_steps))\n",
        "    print('\\t - Validation Accuracy:  {:.4f}'.format(sum(val_accuracy)/len(val_accuracy)))\n",
        "    print('\\t - Validation Precision: {:.4f}'.format(sum(val_precision)/len(val_precision)) if len(val_precision)>0 else '\\t - Validation Precision: NaN')\n",
        "    print('\\t - Validation Recall:    {:.4f}'.format(sum(val_recall)/len(val_recall)) if len(val_recall)>0 else '\\t - Validation Recall: NaN')\n",
        "    print('\\t - Validation F-measure: {:.4f}\\n'.format(sum(val_f1)/len(val_f1)) if len(val_f1)>0 else '\\t - Validation F-measure: NaN')"
      ],
      "metadata": {
        "id": "2s1KmWQrT-SX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tracking variables\n",
        "test_accuracy = []\n",
        "test_precision = []\n",
        "test_recall = []\n",
        "test_f1 = []\n",
        "\n",
        "for batch in test_dataloader:\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "    with torch.no_grad():\n",
        "      # Forward pass\n",
        "      test_output = model(b_input_ids,\n",
        "                          token_type_ids = None,\n",
        "                          attention_mask = b_input_mask)\n",
        "    logits = test_output.logits.detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "    # Calculate validation metrics\n",
        "    b_accuracy, b_precision, b_recall, b_f1 = b_metrics(logits, label_ids)\n",
        "    test_accuracy.append(b_accuracy)\n",
        "    # Update precision only when (tp + fp) !=0; ignore nan\n",
        "    if b_precision != 'nan': test_precision.append(b_precision)\n",
        "    # Update recall only when (tp + fn) !=0; ignore nan\n",
        "    if b_recall != 'nan': test_recall.append(b_recall)\n",
        "    # Update specificity only when (tn + fp) !=0; ignore nan\n",
        "    if b_f1 != 'nan': test_f1.append(b_f1)\n",
        "\n",
        "print('\\t - Testing Accuracy:  {:.4f}'.format(sum(test_accuracy)/len(test_accuracy)))\n",
        "print('\\t - Testing Precision: {:.4f}'.format(sum(test_precision)/len(test_precision)) if len(test_precision)>0 else '\\t - Testing Precision: NaN')\n",
        "print('\\t - Testing Recall:    {:.4f}'.format(sum(test_recall)/len(test_recall)) if len(test_recall)>0 else '\\t - Testing Recall: NaN')\n",
        "print('\\t - Testing F-measure: {:.4f}\\n'.format(sum(test_f1)/len(test_f1)) if len(test_f1)>0 else '\\t - Testing F-measure: NaN')\n",
        "\n"
      ],
      "metadata": {
        "id": "fXtIFFe4UH0a"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}